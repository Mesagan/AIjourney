{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:42px; text-align:center; margin-bottom:30px;\"><span style=\"color:SteelBlue\">Module 4:</span> Regression Algorithm</h1>\n",
    "<hr>\n",
    "\n",
    "Welcome to the workbook for <span style=\"color:royalblue\">Module 4: Regression Algorithms</span>!\n",
    "\n",
    "We'll introduce two powerful mechanisms in modern algorithms: regularization and ensembles. As you'll see, these mechanisms \"fix\" some fatal flaws in older methods, which has lead to their popularity.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "\n",
    "y = mx + c\n",
    "\n",
    "y = dependent variable\n",
    "x = dependent variable\n",
    "m, c = constants/co-efficients\n",
    "\n",
    "y = m1x1 + m2x2 + m3x3 + m4x4 + m5x5.... + c (For Multiple Linear Regression)\n",
    "\n",
    "\n",
    "<br><hr id=\"toc\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Pick ML Algorithms\n",
    "\n",
    "\n",
    "In this lesson, we'll introduce 5 very effective machine learning algorithms for regression tasks. They each have classification counterparts as well.\n",
    "\n",
    "And yes, just 5 for now. Instead of giving you a long list of algorithms, our goal is to explain a few essential concepts (e.g. regularization, ensembling, automatic feature selection) that will teach you why some algorithms tend to perform better than others.\n",
    "\n",
    "In applied machine learning, individual algorithms should be swapped in and out depending on which performs best for the problem and the dataset. Therefore, we will focus on intuition and practical benefits over math and theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Linear Regression is Flawed\n",
    "\n",
    "To introduce the reasoning for some of the advanced algorithms, let's start by discussing basic linear regression. Linear regression models are very common, yet deeply flawed.\n",
    "\n",
    "Linear Regression\n",
    "Simple linear regression models fit a \"straight line\" (technically a hyperplane depending on the number of features, but it's the same idea). In practice, they rarely perform well. We actually recommend skipping them for most machine learning problems.\n",
    "\n",
    "Their main advantage is that they are easy to interpret and understand. However, our goal is not to study the data and write a research report. Our goal is to build a model that can make accurate predictions.\n",
    "\n",
    "In this regard, simple linear regression suffers from two major flaws:\n",
    "\n",
    "1. It's prone to overfit with many input features.\n",
    "2. It cannot easily express non-linear relationships.\n",
    "\n",
    "\n",
    "![220px-Linear_regression.svg.png](attachment:220px-Linear_regression.svg.png)\n",
    "\n",
    "![noisy-sine-linear-regression.png](attachment:noisy-sine-linear-regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at how we can address the first flaw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in Machine Learning\n",
    "\n",
    "\n",
    "This is the first \"advanced\" tactic for improving model performance. It’s considered pretty \"advanced\" in many ML courses, but it’s really pretty easy to understand and implement.\n",
    "\n",
    "The first flaw of linear models is that they are prone to be overfit with many input features.\n",
    "\n",
    "The number of features is too damn high!\n",
    "Let's take an extreme example to illustrate why this happens:\n",
    "\n",
    "Let's say you have 100 observations in your training dataset.\n",
    "Let's say you also have 100 features.\n",
    "If you fit a linear regression model with all of those 100 features, you can perfectly \"memorize\" the training set.\n",
    "Each coefficient would simply memorize one observation. This model would have perfect accuracy on the training data, but perform poorly on unseen data.\n",
    "It hasn’t learned the true underlying patterns; it has only memorized the noise in the training data.\n",
    "Regularization is a technique used to prevent overfitting by artificially penalizing model coefficients.\n",
    "\n",
    "1. It can discourage large coefficients (by dampening them).\n",
    "2. It can also remove features entirely (by setting their coefficients to 0).\n",
    "\n",
    "The \"strength\" of the penalty is tunable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Regression Algos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "Lasso, or LASSO, stands for Least Absolute Shrinkage and Selection Operator.\n",
    "\n",
    "Lasso regression penalizes the absolute size of coefficients.\n",
    "\n",
    "Practically, this leads to coefficients that can be exactly 0.\n",
    "\n",
    "Thus, Lasso offers automatic feature selection because it can completely remove some features.\n",
    "Remember, the \"strength\" of the penalty should be tuned.\n",
    "A stronger penalty leads to more coefficients pushed to zero.\n",
    "\n",
    "# Ridge Regression\n",
    "\n",
    "Ridge stands Really Intense Dangerous Grapefruit Eating (just kidding... it's just ridge).\n",
    "\n",
    "Ridge regression penalizes the squared size of coefficients.\n",
    "\n",
    "Practically, this leads to smaller coefficients, but it doesn't force them to 0.\n",
    "\n",
    "In other words, Ridge offers feature shrinkage.\n",
    "\n",
    "Again, the \"strength\" of the penalty should be tuned.\n",
    "A stronger penalty leads to coefficients pushed closer to zero.\n",
    "\n",
    "# Elastic Net Regression\n",
    "\n",
    "Elastic-Net is a compromise between Lasso and Ridge.\n",
    "\n",
    "Elastic-Net penalizes a mix of both absolute and squared size.\n",
    "\n",
    "The ratio of the two penalty types should be tuned.\n",
    "\n",
    "The overall strength should also be tuned.\n",
    "\n",
    "Oh and in case you’re wondering, there’s no \"best\" type of penalty. It really depends on the dataset and the problem. We recommend trying different algorithms that use a range of penalty strengths as part of the tuning process, which we'll cover in detail tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome, we’ve just seen 3 algorithms that can protect linear regression from overfitting. \n",
    "\n",
    "But if you remember, linear regression suffers from two main flaws:\n",
    "\n",
    "1. It's prone to overfit with many input features.\n",
    "2. It cannot easily express non-linear relationships.\n",
    "\n",
    "How can we address the second flaw?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we need to move away from linear models to do so.... we need to bring in a new category of algorithms.\n",
    "\n",
    "Decision trees model data as a \"tree\" of hierarchical branches. They make branches until they reach \"leaves\" that represent predictions.\n",
    "\n",
    "Decision Tree Example\n",
    "Due to their branching structure, decision trees can easily model nonlinear relationships.\n",
    "\n",
    "For example, let's say for Single Family homes, larger lots command higher prices.\n",
    "However, let's say for Apartments, smaller lots command higher prices (i.e. it's a proxy for urban / rural).\n",
    "This reversal of correlation is difficult for linear models to capture unless you explicitly add an interaction term (i.e. you can anticipate it ahead of time).\n",
    "On the other hand, decision trees can capture this relationship naturally.\n",
    "Unfortunately, decision trees suffer from a major flaw as well. If you allow them to grow limitlessly, they can completely \"memorize\" the training data, just from creating more and more and more branches.\n",
    "\n",
    "As a result, individual unconstrained decision trees are very prone to being overfit.​\n",
    "\n",
    "So, how can we take advantage of the flexibility of decision trees while preventing them from overfitting the training data?\n",
    "\n",
    "![Decision-Tree-Example.jpg](attachment:Decision-Tree-Example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we need to move away from linear models to do so.... we need to bring in a new category of algorithms.\n",
    "\n",
    "Decision trees model data as a \"tree\" of hierarchical branches. They make branches until they reach \"leaves\" that represent predictions.\n",
    "\n",
    "Decision Tree Example\n",
    "Due to their branching structure, decision trees can easily model nonlinear relationships.\n",
    "\n",
    "For example, let's say for Single Family homes, larger lots command higher prices.\n",
    "However, let's say for Apartments, smaller lots command higher prices (i.e. it's a proxy for urban / rural).\n",
    "This reversal of correlation is difficult for linear models to capture unless you explicitly add an interaction term (i.e. you can anticipate it ahead of time).\n",
    "On the other hand, decision trees can capture this relationship naturally.\n",
    "Unfortunately, decision trees suffer from a major flaw as well. If you allow them to grow limitlessly, they can completely \"memorize\" the training data, just from creating more and more and more branches.\n",
    "\n",
    "As a result, individual unconstrained decision trees are very prone to being overfit.​\n",
    "\n",
    "So, how can we take advantage of the flexibility of decision trees while preventing them from overfitting the training data?\n",
    "\n",
    "![Decision-Tree-Example.jpg](attachment:Decision-Tree-Example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Ensembles\n",
    "Ensembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are:\n",
    "\n",
    "### Bagging\n",
    "Bagging attempts to reduce the chance overfitting complex models.\n",
    "\n",
    "It trains a large number of \"strong\" learners in parallel.\n",
    "A strong learner is a model that's relatively unconstrained.\n",
    "Bagging then combines all the strong learners together in order to \"smooth out\" their predictions.\n",
    "\n",
    "### Boosting\n",
    "Boosting attempts to improve the predictive flexibility of simple models.\n",
    "\n",
    "It trains a large number of \"weak\" learners in sequence.\n",
    "A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree).\n",
    "Each one in the sequence focuses on learning from the mistakes of the one before it.\n",
    "Boosting then combines all the weak learners into a single strong learner.\n",
    "\n",
    "\n",
    "While bagging and boosting are both ensemble methods, they approach the problem from opposite directions. Bagging uses complex base models and tries to \"smooth out\" their predictions, while boosting uses simple base models and tries to \"boost\" their aggregate complexity.\n",
    "\n",
    "Ensembling is a general term, but when the base models are decision trees, they have special names: random forests and boosted trees!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests\n",
    "Random forests train a large number of \"strong\" decision trees and combine their predictions through bagging.\n",
    "\n",
    "In addition, there are two sources of \"randomness\" for random forests:\n",
    "\n",
    "Each tree is only allowed to choose from a random subset of features to split on (leading to feature selection).\n",
    "Each tree is only trained on a random subset of observations (a process called resampling).\n",
    "\n",
    "### Boosted trees\n",
    "Boosted trees train a sequence of \"weak\", constrained decision trees and combine their predictions through boosting.\n",
    "\n",
    "Each tree is allowed a maximum depth, which should be tuned.\n",
    "Each tree in the sequence tries to correct the prediction errors of the one before it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
